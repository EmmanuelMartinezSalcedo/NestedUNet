{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e446b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "import albumentations as A\n",
    "from albumentations.core.composition import Compose\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, Sampler, Subset\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import archs\n",
    "import losses\n",
    "from dataset import Dataset\n",
    "from metrics import iou_score\n",
    "from utils import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "print(torch.__file__)\n",
    "print(torch.__version__)\n",
    "\n",
    "print(\"Librerías importadas correctamente\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a348db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingBCELoss(nn.Module):\n",
    "  def __init__(self, smoothing=0.05):\n",
    "    super().__init__()\n",
    "    self.smoothing = smoothing\n",
    "      \n",
    "  def forward(self, pred, target):\n",
    "    target_smooth = target * (1 - self.smoothing) + 0.5 * self.smoothing\n",
    "    return F.binary_cross_entropy(pred, target_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDiceLoss(nn.Module):\n",
    "  def __init__(self, smooth=1.0):\n",
    "    super().__init__()\n",
    "    self.smooth = smooth\n",
    "      \n",
    "  def forward(self, pred, target):\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "    \n",
    "    return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75beadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TverskyLoss(nn.Module):\n",
    "  def __init__(self, alpha=0.5, beta=0.5, smooth=1):\n",
    "    super().__init__()\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.smooth = smooth\n",
    "\n",
    "  def forward(self, pred, target):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred_flat = pred.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "\n",
    "    TP = (pred_flat * target_flat).sum()\n",
    "    FP = ((1 - target_flat) * pred_flat).sum()\n",
    "    FN = (target_flat * (1 - pred_flat)).sum()\n",
    "\n",
    "    tversky = (TP + self.smooth) / (TP + self.alpha * FP + self.beta * FN + self.smooth)\n",
    "    return 1 - tversky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEDiceLoss(nn.Module):\n",
    "  def __init__(self, bce_weight=0.5, dice_weight=0.5, smooth=1.0):\n",
    "    super().__init__()\n",
    "    self.bce_weight = bce_weight\n",
    "    self.dice_weight = dice_weight\n",
    "    self.bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([3.0]))\n",
    "    self.dice = BinaryDiceLoss(smooth=smooth)\n",
    "      \n",
    "  def forward(self, pred, target):\n",
    "    bce_loss = self.bce(pred, target)\n",
    "    \n",
    "    pred_probs = torch.sigmoid(pred)\n",
    "    dice_loss = self.dice(pred_probs, target)\n",
    "    \n",
    "    return self.bce_weight * bce_loss + self.dice_weight * dice_loss\n",
    "\n",
    "print(\"Clases de pérdida definidas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff23ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  def __init__(self, patience=10, min_delta=0.001, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_score = None\n",
    "    self.counter = 0\n",
    "    self.best_weights = None\n",
    "      \n",
    "  def __call__(self, val_score, model):\n",
    "    if self.best_score is None:\n",
    "      self.best_score = val_score\n",
    "      self.save_checkpoint(model)\n",
    "    elif val_score < self.best_score + self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_weights)\n",
    "        return True\n",
    "    else:\n",
    "      self.best_score = val_score\n",
    "      self.save_checkpoint(model)\n",
    "      self.counter = 0\n",
    "    return False\n",
    "  \n",
    "  def save_checkpoint(self, model):\n",
    "    self.best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(\"Early Stopping definido correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'name': None,\n",
    "  'epochs': 100,\n",
    "  'batch_size': 4,\n",
    "  'epoch_subset_size': 10000,\n",
    "  \n",
    "  # Modelo\n",
    "  'arch': 'UNet',\n",
    "  'deep_supervision': False,\n",
    "  'input_channels': 1,\n",
    "  'input_w': 128,\n",
    "  'input_h': 128,\n",
    "  \n",
    "  # Pérdida\n",
    "  'loss': 'TverskyLoss',\n",
    "  \n",
    "  # Dataset\n",
    "  'dataset': 'LUNA16',\n",
    "  'img_ext': '.png',\n",
    "  'mask_ext': '.png',\n",
    "  \n",
    "  # Optimizador\n",
    "  'optimizer': 'Adam',\n",
    "  'lr': 5e-4,\n",
    "  'momentum': 0.9,\n",
    "  'weight_decay': 5e-4,\n",
    "  'nesterov': False,\n",
    "  \n",
    "  # Scheduler\n",
    "  'scheduler': 'OneCycleLR',\n",
    "  'min_lr': 1e-6,\n",
    "  'factor': 0.5,\n",
    "  'patience': 5,\n",
    "  'milestones': '30,60,90',\n",
    "  'gamma': 0.5,\n",
    "  'early_stopping': 10,\n",
    "  \n",
    "  # Anti-overfitting\n",
    "  'dropout_rate': 0.5,\n",
    "  'label_smoothing': 0.0,\n",
    "  'validation_split': 0.2,\n",
    "  'accumulation_steps': 2,\n",
    "  'multiscale_validation': True,\n",
    "  \n",
    "  'num_workers': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['name'] is None:\n",
    "  if config['deep_supervision']:\n",
    "    config['name'] = f\"{config['dataset']}_{config['arch']}_binary_wDS\"\n",
    "  else:\n",
    "    config['name'] = f\"{config['dataset']}_{config['arch']}_binary_woDS\"\n",
    "\n",
    "# Directorio para guardar el modelo\n",
    "print(config['name'])\n",
    "os.makedirs(f\"models/{config['name']}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CONFIGURACIÓN DEL ENTRENAMIENTO:\")\n",
    "print(\"-\" * 50)\n",
    "for key, value in config.items():\n",
    "  print(f\"{key:25}: {value}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Guardar configuración\n",
    "with open(f\"models/{config['name']}/config.yml\", 'w') as f:\n",
    "  yaml.dump(config, f)\n",
    "\n",
    "print(f\"Configuración guardada en models/{config['name']}/config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['loss'] == 'BCELoss':\n",
    "  if config['label_smoothing'] > 0:\n",
    "    criterion = LabelSmoothingBCELoss(config['label_smoothing']).cuda()\n",
    "  else:\n",
    "    criterion = nn.BCELoss().cuda()\n",
    "elif config['loss'] == 'BCEDiceLoss':\n",
    "  criterion = BCEDiceLoss().cuda()\n",
    "elif config['loss'] == 'BinaryDiceLoss':\n",
    "  criterion = BinaryDiceLoss().cuda()\n",
    "elif config['loss'] == 'BCEWithLogitsLoss':\n",
    "  criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "elif config['loss'] == 'TverskyLoss':\n",
    "  criterion = TverskyLoss(alpha=0.7, beta=0.3).cuda()\n",
    "else:\n",
    "  criterion = losses.__dict__[config['loss']]().cuda()\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creando modelo {config['arch']} para segmentación binaria...\")\n",
    "\n",
    "if config['arch'] == 'NestedUNet':\n",
    "  model = archs.__dict__[config['arch']](\n",
    "    input_channels=config['input_channels'],\n",
    "    deep_supervision=config['deep_supervision'],\n",
    "    dropout_rate=config['dropout_rate']\n",
    "  )\n",
    "else:\n",
    "  model = archs.__dict__[config['arch']](\n",
    "    num_classes=1,\n",
    "    input_channels=config['input_channels'],\n",
    "    deep_supervision=config['deep_supervision']\n",
    "  )\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d57910",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Modelo creado exitosamente\")\n",
    "print(f\"Parámetros totales: {total_params:,}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params:,}\")\n",
    "print(f\"Memoria estimada: ~{(total_params * 4) / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7134f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "if config['optimizer'] == 'Adam':\n",
    "  optimizer = optim.Adam(\n",
    "    params, lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "elif config['optimizer'] == 'SGD':\n",
    "  optimizer = optim.SGD(\n",
    "    params, lr=config['lr'], momentum=config['momentum'],\n",
    "    nesterov=config['nesterov'], weight_decay=config['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88997538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_ids(stage_subset, config):\n",
    "  img_path = os.path.normpath(os.path.join('processed', config['dataset'], stage_subset, \"images\", f\"*{config['img_ext']}\"))\n",
    "  img_files = glob(img_path)\n",
    "  img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_files]\n",
    "  \n",
    "  print(f\"Buscando imágenes en: {img_path}\")\n",
    "  print(f\"Imágenes encontradas en {stage_subset}: {len(img_ids)}\")\n",
    "  \n",
    "  if len(img_ids) == 0:\n",
    "    print(f\"No se encontraron imágenes en '{stage_subset}'. Verifica la ruta.\")\n",
    "  else:\n",
    "    print(f\"{stage_subset.replace('-', ' ').title()} cargado correctamente\")\n",
    "    print(f\"   Ejemplo de archivos: {img_ids[:3]}\")\n",
    "  \n",
    "  return img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_ids = get_img_ids('stage1-train', config)\n",
    "val_img_ids = get_img_ids('stage2-val', config)\n",
    "\n",
    "print(f\"División del dataset:\")\n",
    "print(f\"   Entrenamiento: {len(train_img_ids)} imágenes\")\n",
    "print(f\"   Validación: {len(val_img_ids)} imágenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    A.RandomRotate90(),\n",
    "    A.HorizontalFlip(),\n",
    "\n",
    "    A.Resize(config['input_h'], config['input_w']),\n",
    "    A.Normalize(mean=(0.0,), std=(1.0,))\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "  A.Resize(config['input_h'], config['input_w']),\n",
    "  A.Normalize(mean=(0.0,), std=(1.0,))\n",
    "])\n",
    "\n",
    "print(\"Transformaciones definidas:\")\n",
    "print(f\"   Entrenamiento: {len(train_transform.transforms)} transformaciones\")\n",
    "print(f\"   Validación: {len(val_transform.transforms)} transformaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Imágenes en entrenamiento: {len(train_img_ids)}\")\n",
    "print(f\"Imágenes en validación: {len(val_img_ids)}\")\n",
    "\n",
    "train_patients = sorted([\n",
    "    os.path.splitext(f)[0] \n",
    "    for f in os.listdir(os.path.join(\"processed\", config[\"dataset\"], \"stage1-train/images\")) \n",
    "    if f.endswith(config['img_ext'])\n",
    "])\n",
    "val_patients = sorted([\n",
    "    os.path.splitext(f)[0] \n",
    "    for f in os.listdir(os.path.join(\"processed\", config[\"dataset\"], \"stage2-val/images\")) \n",
    "    if f.endswith(config['img_ext'])\n",
    "])\n",
    "\n",
    "train_dataset = Dataset(\n",
    "  img_ids=train_patients,\n",
    "  img_dir=os.path.join(\"processed\", config[\"dataset\"], \"stage1-train/images\"),\n",
    "  mask_dir=os.path.join(\"processed\", config[\"dataset\"], \"stage1-train/masks\"),\n",
    "  img_ext=config[\"img_ext\"],\n",
    "  mask_ext=config[\"mask_ext\"],\n",
    "  transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = Dataset(\n",
    "  img_ids=val_patients,\n",
    "  img_dir=os.path.join(\"processed\", config[\"dataset\"], \"stage2-val/images\"),\n",
    "  mask_dir=os.path.join(\"processed\", config[\"dataset\"], \"stage2-val/masks\"),\n",
    "  img_ext=config[\"img_ext\"],\n",
    "  mask_ext=config[\"mask_ext\"],\n",
    "  transform=val_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=config['batch_size'],\n",
    "  shuffle=True,\n",
    "  num_workers=config['num_workers'],\n",
    "  drop_last=False,\n",
    "  pin_memory=True,\n",
    "  persistent_workers=True,\n",
    "  prefetch_factor=4,\n",
    "  multiprocessing_context='spawn'\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "  val_dataset,\n",
    "  batch_size=config['batch_size'],\n",
    "  shuffle=False,\n",
    "  num_workers=config['num_workers'],\n",
    "  drop_last=False,\n",
    "  pin_memory=True,\n",
    "  persistent_workers=True,\n",
    "  prefetch_factor=4,\n",
    "  multiprocessing_context='spawn'\n",
    ")\n",
    "\n",
    "print(\"DataLoaders creados:\")\n",
    "print(f\"   Batches de entrenamiento: {len(train_loader)}\")\n",
    "print(f\"   Batches de validación: {len(val_loader)}\")\n",
    "print(f\"   Tamaño de batch: {config['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['scheduler'] == 'CosineAnnealingLR':\n",
    "  scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config['epochs'], eta_min=config['min_lr'])\n",
    "elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "  scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=config['factor'], \n",
    "    patience=config['patience'], min_lr=config['min_lr'])\n",
    "elif config['scheduler'] == 'MultiStepLR':\n",
    "  scheduler = lr_scheduler.MultiStepLR(\n",
    "    optimizer, \n",
    "    milestones=[int(e) for e in config['milestones'].split(',')], \n",
    "    gamma=config['gamma'])\n",
    "elif config['scheduler'] == 'OneCycleLR':\n",
    "  steps_per_epoch = len(train_loader) // config['accumulation_steps']\n",
    "  \n",
    "  scheduler = lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=config.get('max_lr', 3e-3),\n",
    "    total_steps=config['epochs'] * steps_per_epoch,\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25,\n",
    "    final_div_factor=1e4,\n",
    "    three_phase=False,\n",
    "  )\n",
    "elif config['scheduler'] == 'ConstantLR':\n",
    "  scheduler = None\n",
    "\n",
    "print(f\"Optimizador: {config['optimizer']} (LR: {config['lr']})\")\n",
    "print(f\"Scheduler: {config['scheduler']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee93c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, masks, infos = next(iter(train_loader))\n",
    "\n",
    "# num_examples = min(4, images.shape[0])\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# for i in range(num_examples):\n",
    "#     # La imagen está en formato [1, H, W], convertimos a [H, W]\n",
    "#     img = images[i].squeeze().cpu().numpy()\n",
    "#     mask = masks[i].squeeze().cpu().numpy()\n",
    "\n",
    "#     # Mostrar imagen\n",
    "#     plt.subplot(2, num_examples, i + 1)\n",
    "#     plt.imshow(img, cmap='gray')\n",
    "#     plt.title(f\"Imagen - {infos['img_id'][i]}\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     # Mostrar máscara\n",
    "#     plt.subplot(2, num_examples, i + 1 + num_examples)\n",
    "#     plt.imshow(mask, cmap='gray')\n",
    "#     plt.title(f\"Máscara - {infos['img_id'][i]}\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb34b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#   sample_batch = next(iter(train_loader))\n",
    "#   print(f\"   Batch de prueba exitoso - Forma: {sample_batch[0].shape}\")\n",
    "# except Exception as e:\n",
    "#   print(f\"   Error en batch de prueba: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSubsetSampler(Sampler):\n",
    "    \"\"\"Sampler que selecciona un subconjunto aleatorio diferente en cada época\"\"\"\n",
    "    \n",
    "    def __init__(self, data_source, subset_size, epoch=None):\n",
    "        self.data_source = data_source\n",
    "        self.subset_size = min(subset_size, len(data_source))\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def set_epoch(self, epoch):\n",
    "        \"\"\"Establece la época actual para cambiar la semilla aleatoria\"\"\"\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.epoch is not None:\n",
    "            random.seed(self.epoch)\n",
    "            torch.manual_seed(self.epoch)\n",
    "        \n",
    "        indices = random.sample(range(len(self.data_source)), self.subset_size)\n",
    "        return iter(indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.subset_size\n",
    "\n",
    "def create_subset_sampler_loader(full_loader, subset_size, config, epoch=None):\n",
    "    original_dataset = full_loader.dataset\n",
    "    \n",
    "    if isinstance(original_dataset, Subset):\n",
    "        original_dataset = original_dataset.dataset\n",
    "    \n",
    "    sampler = RandomSubsetSampler(original_dataset, subset_size, epoch)\n",
    "    \n",
    "    subset_loader = DataLoader(\n",
    "        original_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        sampler=sampler,\n",
    "        num_workers=config.get('num_workers', 4),\n",
    "        pin_memory=config.get('pin_memory', True),\n",
    "        drop_last=config.get('drop_last', False)\n",
    "    )\n",
    "    \n",
    "    return subset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc952d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(config, train_loader, model, criterion, optimizer, epoch):\n",
    "  \"\"\"Entrena el modelo por una época\"\"\"\n",
    "  avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter()}\n",
    "  \n",
    "  model.train()\n",
    "  pbar = tqdm(total=len(train_loader), desc=f'Epoch {epoch}')\n",
    "  \n",
    "  # Gradient accumulation\n",
    "  accumulation_steps = max(1, config['accumulation_steps'] // config['batch_size'])\n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  for i, (input, target, *_) in enumerate(train_loader):\n",
    "    input = input.cuda()\n",
    "    target = target.cuda()\n",
    "\n",
    "    # if i == 0:\n",
    "    #   with profile(\n",
    "    #     activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    #     record_shapes=True,\n",
    "    #     profile_memory=True,\n",
    "    #     with_flops=True\n",
    "    #   ) as prof:\n",
    "    #     with record_function(\"model_training_step\"):\n",
    "    #       # Normalizar target si es necesario\n",
    "    #       if target.max() > 1.0:\n",
    "    #         target = target.float() / 255.0\n",
    "\n",
    "    #       if config['deep_supervision']:\n",
    "    #         outputs = model(input)\n",
    "    #         loss = 0\n",
    "    #         for j, output in enumerate(outputs):\n",
    "    #           weight = 1.0 / (2 ** j)\n",
    "    #           loss += weight * criterion(output, target)\n",
    "    #         loss /= sum([1.0 / (2 ** j) for j in range(len(outputs))])\n",
    "    #         iou = iou_score(outputs[-1], target)\n",
    "    #       else:\n",
    "    #         output = model(input)\n",
    "    #         loss = criterion(output, target)\n",
    "    #         iou = iou_score(output, target)\n",
    "\n",
    "    #       loss = loss / accumulation_steps\n",
    "    #       loss.backward()\n",
    "    #   print(\"\\n[torch.profiler] Métricas primer batch:\")\n",
    "    #   print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=8))\n",
    "    #   print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=8))\n",
    "    #   try:\n",
    "    #     print(prof.key_averages().table(sort_by=\"flops\", row_limit=8))\n",
    "    #   except Exception:\n",
    "    #     print(\"FLOPs no disponibles (requiere PyTorch >= 2.0 y soporte experimental).\")\n",
    "    #   continue\n",
    "\n",
    "    # Normalizar target si es necesario\n",
    "    if target.max() > 1.0:\n",
    "      target = target.float() / 255.0\n",
    "    \n",
    "    # Forward pass\n",
    "    if config['deep_supervision']:\n",
    "      outputs = model(input)\n",
    "      loss = 0\n",
    "      for j, output in enumerate(outputs):\n",
    "        weight = 1.0 / (2 ** j)\n",
    "        loss += weight * criterion(output, target)\n",
    "      loss /= sum([1.0 / (2 ** j) for j in range(len(outputs))])\n",
    "      iou = iou_score(outputs[-1], target)\n",
    "    else:\n",
    "      output = model(input)\n",
    "      loss = criterion(output, target)\n",
    "      iou = iou_score(output, target)\n",
    "    \n",
    "    loss = loss / accumulation_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "    \n",
    "    avg_meters['loss'].update(loss.item() * accumulation_steps, input.size(0))\n",
    "    avg_meters['iou'].update(iou, input.size(0))\n",
    "    \n",
    "    pbar.set_postfix({\n",
    "      'loss': f\"{avg_meters['loss'].avg:.4f}\",\n",
    "      'iou': f\"{avg_meters['iou'].avg:.4f}\"\n",
    "    })\n",
    "    pbar.update(1)\n",
    "  \n",
    "  pbar.close()\n",
    "  return OrderedDict([('loss', avg_meters['loss'].avg), ('iou', avg_meters['iou'].avg)])\n",
    "\n",
    "def validate_epoch(config, val_loader, model, criterion):\n",
    "  \"\"\"Valida el modelo\"\"\"\n",
    "  avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter()}\n",
    "  \n",
    "  model.eval()\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    pbar = tqdm(total=len(val_loader), desc='Validation')\n",
    "    for input, target, *_ in val_loader:\n",
    "      input = input.cuda()\n",
    "      target = target.cuda()\n",
    "      \n",
    "      if target.max() > 1.0:\n",
    "        target = target / 255.0\n",
    "      \n",
    "      if config['deep_supervision']:\n",
    "        outputs = model(input)\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "          loss += criterion(output, target)\n",
    "        loss /= len(outputs)\n",
    "        iou = iou_score(outputs[-1], target)\n",
    "      else:\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        iou = iou_score(output, target)\n",
    "      \n",
    "      avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "      avg_meters['iou'].update(iou, input.size(0))\n",
    "      \n",
    "      pbar.set_postfix({\n",
    "        'val_loss': f\"{avg_meters['loss'].avg:.4f}\",\n",
    "        'val_iou': f\"{avg_meters['iou'].avg:.4f}\"\n",
    "      })\n",
    "      pbar.update(1)\n",
    "    pbar.close()\n",
    "  \n",
    "  return OrderedDict([('loss', avg_meters['loss'].avg), ('iou', avg_meters['iou'].avg)])\n",
    "\n",
    "print(\"Funciones de entrenamiento y validación definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_epoch_sampling(config, train_loader, model, criterion, optimizer, epoch):\n",
    "    epoch_subset_size = config.get('epoch_subset_size')\n",
    "    if epoch_subset_size is None or epoch_subset_size <= 0:\n",
    "        return train_epoch(config, train_loader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    subset_loader = create_subset_sampler_loader(\n",
    "        train_loader, \n",
    "        epoch_subset_size, \n",
    "        config, \n",
    "        epoch=epoch\n",
    "    )\n",
    "    \n",
    "    print(f\"Época {epoch}: Entrenando con subconjunto de {len(subset_loader.sampler)} muestras \"\n",
    "          f\"({len(subset_loader)} batches)\")\n",
    "    \n",
    "    return train_epoch(config, subset_loader, model, criterion, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(log, save_path=None):\n",
    "  \"\"\"Visualiza el progreso del entrenamiento\"\"\"\n",
    "  fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "  \n",
    "  epochs = log['epoch']\n",
    "  \n",
    "  # Loss\n",
    "  ax1.plot(epochs, log['loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "  ax1.plot(epochs, log['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "  ax1.set_title('Training and Validation Loss')\n",
    "  ax1.set_xlabel('Epoch')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  ax1.legend()\n",
    "  ax1.grid(True, alpha=0.3)\n",
    "  \n",
    "  # IoU\n",
    "  ax2.plot(epochs, log['iou'], 'b-', label='Train IoU', linewidth=2)\n",
    "  ax2.plot(epochs, log['val_iou'], 'r-', label='Val IoU', linewidth=2)\n",
    "  ax2.set_title('Training and Validation IoU')\n",
    "  ax2.set_xlabel('Epoch')\n",
    "  ax2.set_ylabel('IoU')\n",
    "  ax2.legend()\n",
    "  ax2.grid(True, alpha=0.3)\n",
    "  \n",
    "  # Learning Rate\n",
    "  ax3.plot(epochs, log['lr'], 'g-', linewidth=2)\n",
    "  ax3.set_title('Learning Rate Schedule')\n",
    "  ax3.set_xlabel('Epoch')\n",
    "  ax3.set_ylabel('Learning Rate')\n",
    "  ax3.set_yscale('log')\n",
    "  ax3.grid(True, alpha=0.3)\n",
    "  \n",
    "  # Overfitting Detection\n",
    "  iou_gap = [train - val for train, val in zip(log['iou'], log['val_iou'])]\n",
    "  ax4.plot(epochs, iou_gap, 'purple', linewidth=2)\n",
    "  ax4.axhline(y=0.08, color='red', linestyle='--', alpha=0.7, label='Overfitting Threshold')\n",
    "  ax4.set_title('Overfitting Detection (Train IoU - Val IoU)')\n",
    "  ax4.set_xlabel('Epoch')\n",
    "  ax4.set_ylabel('IoU Gap')\n",
    "  ax4.legend()\n",
    "  ax4.grid(True, alpha=0.3)\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  \n",
    "  if save_path:\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "  \n",
    "  plt.show()\n",
    "\n",
    "print(\"Función de visualización definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = OrderedDict([\n",
    "  ('epoch', []),\n",
    "  ('lr', []),\n",
    "  ('loss', []),\n",
    "  ('iou', []),\n",
    "  ('val_loss', []),\n",
    "  ('val_iou', []),\n",
    "])\n",
    "\n",
    "best_iou = 0\n",
    "early_stopping = EarlyStopping(patience=config['early_stopping'], min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca310dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_TO_RUN = 100\n",
    "\n",
    "current_epoch = len(log['epoch'])\n",
    "end_epoch = min(current_epoch + EPOCHS_TO_RUN, config['epochs'])\n",
    "\n",
    "for epoch in range(current_epoch, end_epoch):\n",
    "  print(f'\\nÉpoca [{epoch + 1}/{config[\"epochs\"]}]')\n",
    "  \n",
    "  # Entrenamiento\n",
    "  train_log = train_with_epoch_sampling(config, train_loader, model, criterion, optimizer, epoch)\n",
    "  \n",
    "  # Validación\n",
    "  val_log = validate_epoch(config, val_loader, model, criterion)\n",
    "  \n",
    "  # Mostrar imágenes\n",
    "  # model.eval()\n",
    "  # with torch.no_grad():\n",
    "  #   sample = next(iter(val_loader))\n",
    "  #   input, target = sample[0].cuda(), sample[1].cuda()\n",
    "  #   output = model(input)\n",
    "\n",
    "  #   if isinstance(output, list):\n",
    "  #       output = output[-1]\n",
    "\n",
    "  #   output = torch.sigmoid(output)\n",
    "  #   pred = (output > 0.5).float()\n",
    "\n",
    "  #   idx = random.randint(0, input.size(0) - 1)\n",
    "\n",
    "  # plt.figure(figsize=(12, 4))\n",
    "  # plt.subplot(1, 3, 1)\n",
    "  # plt.imshow(input[idx, 0].cpu(), cmap='gray')\n",
    "  # plt.title(f'Input (Epoch {epoch + 1})')\n",
    "\n",
    "  # plt.subplot(1, 3, 2)\n",
    "  # plt.imshow(target[idx, 0].cpu(), cmap='gray')\n",
    "  # plt.title('Target')\n",
    "\n",
    "  # plt.subplot(1, 3, 3)\n",
    "  # plt.imshow(pred[idx, 0].cpu(), cmap='gray')\n",
    "  # plt.title('Predicción')\n",
    "  # plt.tight_layout()\n",
    "  # plt.show()\n",
    "\n",
    "  # Actualizar scheduler\n",
    "  if config['scheduler'] == 'CosineAnnealingLR':\n",
    "    scheduler.step()\n",
    "  elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "    scheduler.step(val_log['loss'])\n",
    "  \n",
    "  # Mostrar resultados\n",
    "  current_lr = optimizer.param_groups[0]['lr']\n",
    "  print(f'Loss: {train_log[\"loss\"]:.4f} | IoU: {train_log[\"iou\"]:.4f} | '\n",
    "        f'Val Loss: {val_log[\"loss\"]:.4f} | Val IoU: {val_log[\"iou\"]:.4f} | LR: {current_lr:.2e}')\n",
    "  \n",
    "  # Detección temprana de overfitting\n",
    "  overfitting_gap = train_log['iou'] - val_log['iou']\n",
    "  loss_gap = val_log['loss'] - train_log['loss']\n",
    "  \n",
    "  if overfitting_gap > 0.08 and loss_gap > 0.1:\n",
    "    print(f'OVERFITTING DETECTADO - IoU gap: {overfitting_gap:.4f}, Loss gap: {loss_gap:.4f}')\n",
    "    for param_group in optimizer.param_groups:\n",
    "      param_group['lr'] *= 0.5\n",
    "    print(f'Learning rate reducido a: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "  \n",
    "  # Actualizar log\n",
    "  log['epoch'].append(epoch)\n",
    "  log['lr'].append(current_lr)\n",
    "  log['loss'].append(train_log['loss'])\n",
    "  log['iou'].append(train_log['iou'])\n",
    "  log['val_loss'].append(val_log['loss'])\n",
    "  log['val_iou'].append(val_log['iou'])\n",
    "  \n",
    "  # Guardar log\n",
    "  pd.DataFrame(log).to_csv(f'models/{config[\"name\"]}/log.csv', index=False)\n",
    "  \n",
    "  # Guardar mejor modelo\n",
    "  if val_log['iou'] > best_iou:\n",
    "    best_iou = val_log['iou']\n",
    "    best_model_path = f'models/{config[\"name\"]}/best_model.pth'\n",
    "    torch.save(model.state_dict(), best_model_path)\n",
    "    print(f'Mejor modelo guardado en {best_model_path} (IoU: {best_iou:.4f})')\n",
    "\n",
    "  # Guardar modelo de esta época\n",
    "  epoch_model_path = f'models/{config[\"name\"]}/checkpoints/model_epoch_{epoch + 1}.pth'\n",
    "  os.makedirs(os.path.dirname(epoch_model_path), exist_ok=True)\n",
    "  torch.save(model.state_dict(), epoch_model_path)\n",
    "  print(f'Modelo de la época {epoch + 1} guardado en {epoch_model_path}')\n",
    "  \n",
    "  # Early stopping\n",
    "  if early_stopping(val_log['iou'], model):\n",
    "    print(\"Early stopping activado\")\n",
    "    break\n",
    "  \n",
    "  # Limpiar memoria\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nEntrenamiento completado hasta época {epoch}\")\n",
    "print(f\"Mejor IoU alcanzado: {best_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(log['epoch']) > 0:\n",
    "  plot_training_progress(log, f'models/{config[\"name\"]}/training_progress.png')\n",
    "  \n",
    "  # Mostrar estadísticas actuales\n",
    "  print(\"ESTADÍSTICAS DEL ENTRENAMIENTO:\")\n",
    "  print(\"-\" * 40) \n",
    "  print(f\"Épocas completadas: {len(log['epoch'])}\")\n",
    "  print(f\"Mejor IoU: {max(log['val_iou']):.4f}\")\n",
    "  print(f\"Menor Loss de validación: {min(log['val_loss']):.4f}\")\n",
    "  print(f\"IoU actual: {log['val_iou'][-1]:.4f}\")\n",
    "  print(f\"Loss actual: {log['val_loss'][-1]:.4f}\")\n",
    "  \n",
    "  # Detectar tendencias\n",
    "  if len(log['val_iou']) >= 3:\n",
    "    recent_trend = np.mean(log['val_iou'][-3:]) - np.mean(log['val_iou'][-6:-3]) if len(log['val_iou']) >= 6 else 0\n",
    "    print(f\"Tendencia reciente: {'Mejorando' if recent_trend > 0 else 'Empeorando' if recent_trend < 0 else 'Estable'}\")\n",
    "else:\n",
    "  print(\"No hay datos de entrenamiento para visualizar. Ejecuta el bloque de entrenamiento primero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eda88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, best_iou, log, filename):\n",
    "  \"\"\"Guardar checkpoint completo\"\"\"\n",
    "  checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "    'best_iou': best_iou,\n",
    "    'log': log,\n",
    "    'config': config\n",
    "  }\n",
    "  torch.save(checkpoint, filename)\n",
    "  print(f\"Checkpoint guardado: {filename}\")\n",
    "\n",
    "def load_checkpoint(filename, model, optimizer, scheduler=None):\n",
    "  \"\"\"Cargar checkpoint completo\"\"\"\n",
    "  checkpoint = torch.load(filename)\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  if scheduler and checkpoint['scheduler_state_dict']:\n",
    "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "  \n",
    "  return (checkpoint['epoch'], checkpoint['best_iou'], \n",
    "          checkpoint['log'], checkpoint['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config['name'])\n",
    "if len(log['epoch']) > 0:\n",
    "  checkpoint_path = f'models/{config[\"name\"]}/checkpoint_epoch_{len(log[\"epoch\"])}.pth'\n",
    "  save_checkpoint(model, optimizer, scheduler, len(log['epoch']), best_iou, log, checkpoint_path)\n",
    "\n",
    "print(\"Funciones de checkpoint definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_final_summary():\n",
    "  \"\"\"Imprime un resumen final del entrenamiento\"\"\"\n",
    "  if len(log['epoch']) == 0:\n",
    "    print(\"No hay datos de entrenamiento para resumir\")\n",
    "    return\n",
    "  \n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"RESUMEN FINAL DEL ENTRENAMIENTO\")\n",
    "  print(\"=\"*60)\n",
    "  \n",
    "  print(f\"Configuración:\")\n",
    "  print(f\"   • Modelo: {config['arch']}\")\n",
    "  print(f\"   • Función de pérdida: {config['loss']}\")\n",
    "  print(f\"   • Optimizador: {config['optimizer']} (LR: {config['lr']})\")\n",
    "  print(f\"   • Batch size: {config['batch_size']}\")\n",
    "  print(f\"   • Early stopping: {config['early_stopping']} épocas\")\n",
    "  \n",
    "  print(f\"\\nResultados:\")\n",
    "  print(f\"   • Épocas completadas: {len(log['epoch'])}/{config['epochs']}\")\n",
    "  print(f\"   • Mejor IoU: {max(log['val_iou']):.4f}\")\n",
    "  print(f\"   • IoU final: {log['val_iou'][-1]:.4f}\")\n",
    "  print(f\"   • Mejor Loss validación: {min(log['val_loss']):.4f}\")\n",
    "  print(f\"   • Loss final: {log['val_loss'][-1]:.4f}\")\n",
    "  \n",
    "  # Análisis de overfitting\n",
    "  final_gap = log['iou'][-1] - log['val_iou'][-1]\n",
    "  print(f\"\\nAnálisis de Overfitting:\")\n",
    "  print(f\"   • Gap IoU (Train-Val): {final_gap:.4f}\")\n",
    "  if final_gap > 0.08:\n",
    "    print(f\"   • Posible overfitting detectado\")\n",
    "  else:\n",
    "    print(f\"   • Nivel de overfitting aceptable\")\n",
    "  \n",
    "  # Tendencia de mejora\n",
    "  if len(log['val_iou']) >= 5:\n",
    "    recent_iou = np.mean(log['val_iou'][-3:])\n",
    "    early_iou = np.mean(log['val_iou'][:3])\n",
    "    improvement = recent_iou - early_iou\n",
    "    print(f\"\\nMejora Total:\")\n",
    "    print(f\"   • IoU inicial: {early_iou:.4f}\")\n",
    "    print(f\"   • IoU reciente: {recent_iou:.4f}\")\n",
    "    print(f\"   • Mejora: {improvement:.4f} ({improvement/early_iou*100:.1f}%)\")\n",
    "  \n",
    "  print(f\"\\nArchivos guardados:\")\n",
    "  print(f\"   • Modelo: models/{config['name']}/model.pth\")\n",
    "  print(f\"   • Log: models/{config['name']}/log.csv\")\n",
    "  print(f\"   • Config: models/{config['name']}/config.yml\")\n",
    "  print(f\"   • Gráficas: models/{config['name']}/training_progress.png\")\n",
    "  \n",
    "  print(\"=\"*60)\n",
    "\n",
    "# Ejecutar resumen si hay datos\n",
    "print_final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
